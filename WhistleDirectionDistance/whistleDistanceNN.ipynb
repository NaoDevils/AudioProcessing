{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T09:34:17.506892700Z",
     "start_time": "2023-05-30T09:34:12.710314700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from scipy import signal\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.initializers import Constant, GlorotUniform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "tf.random.set_seed(9)\n",
    "\n",
    "X_TRAIN = np.load(\"direction_x_train.npy\", mmap_mode=\"r\")\n",
    "X_TRAIN_PHI = np.load(\"direction_x_train_phi.npy\", mmap_mode=\"r\")\n",
    "X_LEVEL_TRAIN = np.load(\"direction_x_level_train.npy\", mmap_mode=\"r\")\n",
    "Y_TRAIN = np.load(\"direction_y_train.npy\", mmap_mode=\"r\")\n",
    "ROOMS_TRAIN = np.load(\"direction_room_train.npy\", mmap_mode=\"r\")\n",
    "whistle_length_train = np.load(\"whistle_length_train.npy\", mmap_mode=\"r\")\n",
    "whistle_gap_length_train = np.load(\"whistle_gap_length_train.npy\", mmap_mode=\"r\")\n",
    "\n",
    "X_TEST = np.load(\"direction_x_test.npy\", mmap_mode=\"r\")\n",
    "X_TEST_PHI = np.load(\"direction_x_test_phi.npy\", mmap_mode=\"r\")\n",
    "X_LEVEL_TEST = np.load(\"direction_x_level_test.npy\", mmap_mode=\"r\")\n",
    "Y_TEST = np.load(\"direction_y_test.npy\", mmap_mode=\"r\")\n",
    "ROOMS_TEST = np.load(\"direction_room_test.npy\", mmap_mode=\"r\")\n",
    "whistle_length_test = np.load(\"whistle_length_test.npy\", mmap_mode=\"r\")\n",
    "whistle_gap_length_test = np.load(\"whistle_gap_length_test.npy\", mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.   4.5  6.75 9.    inf]\n"
     ]
    }
   ],
   "source": [
    "BINS = np.array([2.0, 4.5, 6.75, 9, np.inf])\n",
    "print(BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MIN_WHISTLE_SIZE = 18\n",
    "GAUSS = signal.gaussian((2*len(BINS))+1, std=0.56)\n",
    "GAUSS = (GAUSS - np.min(GAUSS))/(np.max(GAUSS) - np.min(GAUSS))\n",
    "GAUSS = GAUSS.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_nearest_bin(data, bins):\n",
    "    tmp_bins = np.copy(bins)\n",
    "    tmp_bins = np.hstack([0.0, tmp_bins])\n",
    "    max_error = np.diff(tmp_bins)/2\n",
    "    bin_centers = tmp_bins[:-1] + max_error\n",
    "    max_error[-1] = bin_centers[-1] = np.max(Y_TRAIN[:,1])\n",
    "    error = np.abs(tmp_bins - data)\n",
    "    scale_error = np.abs(bin_centers - data)\n",
    "    scale = scale_error[np.digitize(data, bins, right=True)] / max_error[np.digitize(data, bins, right=True)]\n",
    "    if np.min(scale_error) == 0:\n",
    "        return None, scale\n",
    "    else:\n",
    "        return np.argmin(error) - 1, scale\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_dampening_mask(value, bins):\n",
    "    i = np.digitize(value, bins, right=True)\n",
    "    j, scale = get_nearest_bin(value, bins)\n",
    "    idxs = np.arange(0, len(bins), 1)\n",
    "    if j is None:\n",
    "        return idxs != i, scale\n",
    "    elif i <= j:\n",
    "        return idxs < i, scale\n",
    "    else:\n",
    "        return idxs > i, scale\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def discretization(data, bins, tol=0.01):\n",
    "    idx = np.digitize(data, BINS, right=True)\n",
    "\n",
    "    encoded_data = np.zeros(shape=BINS.shape, dtype=np.float32)\n",
    "    encoded_data = np.copy(GAUSS[len(BINS)-idx:(len(BINS)-idx)+len(BINS)])\n",
    "    mask, scale = get_dampening_mask(data, bins)\n",
    "    encoded_data[mask] *= (1 + (tol*scale)) - scale\n",
    "    mask[idx] = True\n",
    "    mask = np.invert(mask)\n",
    "    encoded_data[mask] *= (1 - (tol*scale)) + scale\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def data_generator(whistle_length, x, y, r, window_size_direct=3, use_dropout=False, use_noise=False, use_scale=False, batch_size=BATCH_SIZE, noise=0.001, scale=0.001, broken_mic_probability=20):\n",
    "    shape = x[0].shape\n",
    "    whistle_length = itertools.cycle(whistle_length)\n",
    "    x = itertools.cycle(x)\n",
    "    y = itertools.cycle(y)\n",
    "    r = itertools.cycle(r)\n",
    "\n",
    "    while True:\n",
    "        X = []\n",
    "        DRR = []\n",
    "        Y_dist = []\n",
    "        ROOMS = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "\n",
    "            # skip too short whistles\n",
    "            length = next(whistle_length)\n",
    "            while length < MIN_WHISTLE_SIZE:\n",
    "                for _ in range(length):\n",
    "                    next(x)\n",
    "                    next(y)\n",
    "                    next(r)\n",
    "                length = next(whistle_length)\n",
    "            \n",
    "            # build spectrogram and drr\n",
    "            spectrogram = []\n",
    "            dist = None\n",
    "            direct = np.zeros(shape=shape)\n",
    "            reverberation = np.zeros(shape=shape)\n",
    "            mic_broken_bool = np.random.random() < (broken_mic_probability/100)\n",
    "            broken_mic = np.random.randint(0, 4)\n",
    "            if use_scale:\n",
    "                scale_mask = np.ones(shape[1])\n",
    "                scale_mask = scale_mask * np.random.normal(loc=0.0, scale=scale)\n",
    "            for i in range(MIN_WHISTLE_SIZE):\n",
    "                amp_slice = np.abs(next(x))\n",
    "                if use_scale:\n",
    "                    amp_slice = amp_slice + scale_mask\n",
    "                    amp_slice = np.clip(amp_slice, K.epsilon(), np.inf)\n",
    "                if use_noise:\n",
    "                    noise_mask = np.ones(shape)\n",
    "                    noise_mask = noise_mask * np.random.normal(loc=0.0, scale=noise, size=shape)\n",
    "                    amp_slice = amp_slice + noise_mask\n",
    "                    amp_slice = np.clip(amp_slice, K.epsilon(), np.inf)\n",
    "                if use_dropout and mic_broken_bool:\n",
    "                    amp_slice[:,broken_mic] = np.ones_like(amp_slice[:,broken_mic]) * K.epsilon()\n",
    "                spectrogram_slice = 20 * np.log10(amp_slice)\n",
    "                \n",
    "                spectrogram.append(spectrogram_slice)\n",
    "\n",
    "                if i < window_size_direct:\n",
    "                    direct += amp_slice\n",
    "                else:\n",
    "                    reverberation += amp_slice\n",
    "\n",
    "                dist = next(y)[1]\n",
    "                room = next(r)\n",
    "                \n",
    "            # skip rest of long whistles\n",
    "            for _ in range(np.abs(length - MIN_WHISTLE_SIZE)):\n",
    "                next(x)\n",
    "                next(y)\n",
    "                next(r)\n",
    "\n",
    "            X.append(spectrogram)\n",
    "            DRR.append(20 * np.log10(direct/reverberation))\n",
    "            Y_dist.append(discretization(dist, tuple(BINS)))\n",
    "            ROOMS.append(room)\n",
    "        yield np.asarray(X),  np.asarray(DRR), np.asarray(Y_dist), np.asarray(ROOMS)\n",
    "\n",
    "GENERATOR = data_generator(whistle_length_train, X_TRAIN, Y_TRAIN, ROOMS_TRAIN, use_dropout=False, use_noise=False, use_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial bias distance: \t[0.3869,0.2901,0.1095,0.1623,0.0512]\n"
     ]
    }
   ],
   "source": [
    "idxs, counts = np.unique(np.digitize(Y_TRAIN[:,1], BINS, right=True), return_counts=True)\n",
    "sum_of_counts = np.sum(counts)\n",
    "initial_bias_distance = np.zeros(BINS.shape, dtype=np.float32)\n",
    "for c_idx, idx in enumerate(idxs):\n",
    "    initial_bias_distance[idx] = counts[c_idx]/sum_of_counts\n",
    "\n",
    "str_init_bias_distance = np.array2string(initial_bias_distance, precision=4, separator=\",\", max_line_width=np.inf, floatmode=\"fixed\")\n",
    "print(f\"initial bias distance: \\t{str_init_bias_distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T09:49:10.183832400Z",
     "start_time": "2023-05-30T09:49:10.157902200Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_backbone2D(shape):\n",
    "    inputs = layers.Input(shape=shape)\n",
    "    augmented = layers.GaussianNoise(stddev=5)(inputs)\n",
    "    dropout = layers.Dropout(rate=0.25, noise_shape=(BATCH_SIZE, 1, 1, 4))(augmented)\n",
    "\n",
    "    conv = layers.Conv2D(filters=64, kernel_size=(3,1), strides=1, data_format='channels_last', padding=\"same\", use_bias=False, kernel_initializer=GlorotUniform)(dropout)\n",
    "    leakyReLU = layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(1,2), strides=(1,2), data_format='channels_first', padding=\"same\")(leakyReLU)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(2,1), strides=(2,1), data_format='channels_first', padding=\"valid\")(maxpool)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(2,1), strides=(2,1), data_format='channels_last', padding=\"valid\")(maxpool)\n",
    "\n",
    "    conv = layers.Conv2D(filters=32, kernel_size=(3,1), strides=1, data_format='channels_last', padding=\"same\", use_bias=False, kernel_initializer=GlorotUniform)(maxpool)\n",
    "    leakyReLU = layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(1,2), strides=(1,2), data_format='channels_first', padding=\"same\")(leakyReLU)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(2,1), strides=(2,1), data_format='channels_first', padding=\"valid\")(maxpool)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(2,1), strides=(2,1), data_format='channels_last', padding=\"valid\")(maxpool)\n",
    "\n",
    "    conv = layers.Conv2D(filters=16, kernel_size=(3,1), strides=1, data_format='channels_last', padding=\"same\", use_bias=False, kernel_initializer=GlorotUniform)(maxpool)\n",
    "    leakyReLU = layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(1,2), strides=(1,2), data_format='channels_first', padding=\"same\")(leakyReLU)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(2,1), strides=(2,1), data_format='channels_first', padding=\"valid\")(maxpool)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(2,1), strides=(2,1), data_format='channels_last', padding=\"valid\")(maxpool)\n",
    "\n",
    "    conv = layers.Conv2D(filters=8, kernel_size=(3,1), strides=1, data_format='channels_last', padding=\"same\", use_bias=False, kernel_initializer=GlorotUniform)(maxpool)\n",
    "    leakyReLU = layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(1,2), strides=(1,2), data_format='channels_first', padding=\"same\")(leakyReLU)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(2,1), strides=(2,1), data_format='channels_first', padding=\"valid\")(maxpool)\n",
    "    maxpool = layers.MaxPool2D(pool_size=(2,1), strides=(2,1), data_format='channels_last', padding=\"valid\")(maxpool)\n",
    "\n",
    "    flatten = layers.Flatten()(maxpool)\n",
    "\n",
    "    dense = layers.Dense(64, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(0.001), use_bias=False, kernel_initializer=GlorotUniform)(flatten)\n",
    "    dropout = layers.Dropout(rate=0.2)(dense)\n",
    "    dense = layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(0.001), use_bias=False, kernel_initializer=GlorotUniform)(dropout)\n",
    "    dropout = layers.Dropout(rate=0.2)(dense)\n",
    "    dense = layers.Dense(16, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(0.001), use_bias=False, kernel_initializer=GlorotUniform)(dropout)\n",
    "    features = layers.Dropout(rate=0.2)(dense)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=features)\n",
    "\n",
    "def create_backbone1D(shape):\n",
    "    inputs = layers.Input(shape=shape)\n",
    "    augmented = layers.GaussianNoise(stddev=0.5)(inputs)\n",
    "    dropout = layers.Dropout(rate=0.25, noise_shape=(BATCH_SIZE, 1, 4))(augmented)\n",
    "\n",
    "    conv = layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", use_bias=False, kernel_initializer=GlorotUniform)(dropout)\n",
    "    leakyReLU = layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    maxpool = layers.MaxPool1D(pool_size=2, strides=2, data_format='channels_first', padding=\"same\")(leakyReLU)\n",
    "    maxpool = layers.MaxPool1D(pool_size=2, strides=2, data_format='channels_last', padding=\"valid\")(maxpool)\n",
    "\n",
    "    conv = layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", use_bias=False, kernel_initializer=GlorotUniform)(maxpool)\n",
    "    leakyReLU = layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    maxpool = layers.MaxPool1D(pool_size=2, strides=2, data_format='channels_first', padding=\"same\")(leakyReLU)\n",
    "    maxpool = layers.MaxPool1D(pool_size=2, strides=2, data_format='channels_last', padding=\"valid\")(maxpool)\n",
    "\n",
    "    conv = layers.Conv1D(filters=16, kernel_size=3, strides=1, padding=\"same\", use_bias=False, kernel_initializer=GlorotUniform)(maxpool)\n",
    "    leakyReLU = layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    maxpool = layers.MaxPool1D(pool_size=2, strides=2, data_format='channels_first', padding=\"same\")(leakyReLU)\n",
    "    maxpool = layers.MaxPool1D(pool_size=2, strides=2, data_format='channels_last', padding=\"valid\")(maxpool)\n",
    "\n",
    "    conv = layers.Conv1D(filters=8, kernel_size=3, strides=1, padding=\"same\", use_bias=False, kernel_initializer=GlorotUniform)(maxpool)\n",
    "    leakyReLU = layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    maxpool = layers.MaxPool1D(pool_size=2, strides=2, data_format='channels_first', padding=\"same\")(leakyReLU)\n",
    "    maxpool = layers.MaxPool1D(pool_size=2, strides=2, data_format='channels_last', padding=\"valid\")(maxpool)\n",
    "\n",
    "    flatten = layers.Flatten()(maxpool)\n",
    "\n",
    "    dense = layers.Dense(64, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(0.001), use_bias=False, kernel_initializer=GlorotUniform)(flatten)\n",
    "    dropout = layers.Dropout(rate=0.2)(dense)\n",
    "    dense = layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(0.001), use_bias=False, kernel_initializer=GlorotUniform)(dropout)\n",
    "    dropout = layers.Dropout(rate=0.2)(dense)\n",
    "    dense = layers.Dense(16, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(0.001), use_bias=False, kernel_initializer=GlorotUniform)(dropout)\n",
    "    features = layers.Dropout(rate=0.2)(dense)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=features)\n",
    "\n",
    "def create_model():\n",
    "    spectrogram = layers.Input(shape=(18, 513, 4))\n",
    "    drr = layers.Input(shape=(513, 4))\n",
    "\n",
    "    spectrogram_backbone = create_backbone2D((18, 513, 4))\n",
    "    spectrogram_features = spectrogram_backbone(spectrogram)\n",
    "\n",
    "    drr_backbone = create_backbone1D((513, 4))\n",
    "    drr_features = drr_backbone(drr)\n",
    "\n",
    "    features = layers.Concatenate()([spectrogram_features, drr_features])\n",
    "\n",
    "    # Feature refinement\n",
    "    dense = layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(0.001), use_bias=False, kernel_initializer=GlorotUniform)(features)\n",
    "    dropout = layers.Dropout(rate=0.2)(dense)\n",
    "    dense = layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(0.001), use_bias=False, kernel_initializer=GlorotUniform)(dropout)\n",
    "    dropout = layers.Dropout(rate=0.2)(dense)\n",
    "    dense = layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(0.001), use_bias=False, kernel_initializer=GlorotUniform)(dropout)\n",
    "    dropout = layers.Dropout(rate=0.2)(dense)\n",
    "\n",
    "    # Conclusion\n",
    "    distance = layers.Dense(5, activation=\"sigmoid\", use_bias=True, bias_initializer=Constant([0.3869,0.2901,0.1095,0.1623,0.0512]), kernel_initializer=GlorotUniform)(dropout)\n",
    "\n",
    "    return tf.keras.Model(inputs=[spectrogram, drr], outputs=distance)\n",
    "\n",
    "tf.random.set_seed(9)\n",
    "GENERATOR = data_generator(whistle_length_train, X_TRAIN, Y_TRAIN, ROOMS_TRAIN)\n",
    "TEST_GENERATOR = data_generator(whistle_length_test, X_TEST, Y_TEST, ROOMS_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T09:49:13.445747300Z",
     "start_time": "2023-05-30T09:49:13.352988700Z"
    }
   },
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T09:49:11.898027600Z",
     "start_time": "2023-05-30T09:49:11.872099600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SCALE = 1\n",
    "GAMMA = 2\n",
    "ALPHA = 0.75\n",
    "\n",
    "@tf.function\n",
    "def dist_loss(y_true, y_pred, scale = SCALE):\n",
    "    # Adapted Focal Loss\n",
    "    p_t = 1 - (tf.abs(y_true - y_pred) / tf.math.maximum(tf.abs(1 - y_true), tf.abs(0 - y_true)))\n",
    "    alpha_t = tf.where(tf.equal(y_true, 1.0), ALPHA, 1 - ALPHA)\n",
    "    error = -alpha_t * tf.math.pow((1 - p_t), GAMMA) * tf.math.log(p_t + K.epsilon())\n",
    "    error = tf.reduce_mean(tf.reduce_sum(error, axis=-1))\n",
    "\n",
    "    return scale * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadam = tf.optimizers.Nadam(learning_rate=3e-4)\n",
    "optimizer = nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(spectrogram, drr, distances):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_distances = model([spectrogram, drr], training=True)\n",
    "\n",
    "        model_loss = dist_loss(distances, pred_distances)\n",
    "\n",
    "        grads = tape.gradient(model_loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))        \n",
    "\n",
    "    return model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2500\n",
    "PATIENCE = 100\n",
    "MIN_DELTA = 0.001\n",
    "TRAIN_STEPS = len(whistle_length_train[whistle_length_train>=MIN_WHISTLE_SIZE])/BATCH_SIZE\n",
    "TEST_STEPS = len(whistle_length_test[whistle_length_test>=MIN_WHISTLE_SIZE])/BATCH_SIZE\n",
    "CRITERIA = \"mean\"\n",
    "\n",
    "mean_epoch_loss = np.inf\n",
    "mean_train_epoch_losses = []\n",
    "mean_validate_epoch_losses = []\n",
    "\n",
    "median_epoch_loss = np.inf\n",
    "median_train_epoch_losses = []\n",
    "median_validate_epoch_losses = []\n",
    "\n",
    "epoch_losses = []\n",
    "min_validate_loss = np.inf\n",
    "used_epochs = EPOCHS\n",
    "no_improvements = 0\n",
    "current_validation_loss = np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    if len(epoch_losses) > 0:\n",
    "        mean_epoch_loss = np.mean(epoch_losses)\n",
    "        mean_train_epoch_losses.append(mean_epoch_loss)\n",
    "\n",
    "        median_epoch_loss = np.median(epoch_losses)\n",
    "        median_train_epoch_losses.append(median_epoch_loss)\n",
    "\n",
    "        validate_losses = []\n",
    "        validate_ranges = []\n",
    "        for step, batch in enumerate(TEST_GENERATOR):\n",
    "            if step >= TEST_STEPS:\n",
    "                    break\n",
    "            spectrogram, drr, distances, rooms = batch\n",
    "            pred_distances = model.predict([spectrogram, drr])\n",
    "            distance_loss = dist_loss(distances, pred_distances)\n",
    "            validate_losses.append(distance_loss)\n",
    "        mean_validate_epoch_losses.append(np.mean(validate_losses))\n",
    "        median_validate_epoch_losses.append(np.median(validate_losses))\n",
    "\n",
    "        if CRITERIA == \"median\":\n",
    "            current_validation_loss = np.median(validate_losses)\n",
    "        else:\n",
    "            current_validation_loss = np.mean(validate_losses)\n",
    "        \n",
    "        if min_validate_loss - current_validation_loss > MIN_DELTA:\n",
    "            no_improvements = 0\n",
    "        else:\n",
    "            no_improvements += 1\n",
    "\n",
    "        if current_validation_loss < min_validate_loss:\n",
    "            min_validate_loss = current_validation_loss\n",
    "            model.save_weights(\"./ckpt.h5\")\n",
    "\n",
    "        if no_improvements > PATIENCE:\n",
    "            used_epochs = epoch\n",
    "            break\n",
    "                  \n",
    "\n",
    "    epoch_losses = []\n",
    "    for step, batch in enumerate(GENERATOR):\n",
    "        if step >= TRAIN_STEPS:\n",
    "            break\n",
    "        spectrogram, drr, distances, rooms = batch\n",
    "        model_loss = train_step(spectrogram, drr, distances)\n",
    "        epoch_losses.append(model_loss)\n",
    "        print(f\"Epoch: {epoch}, Epoch Loss: {round(float(mean_epoch_loss),4)} | Validation Loss: {round(float(current_validation_loss),4)} | Training Loss: {round(float(model_loss),4)}         \\r\", end=\"\")\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    import winsound\n",
    "    winsound.PlaySound(\"SystemHand\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize =(20, 7))\n",
    "\n",
    "if CRITERIA == \"median\":\n",
    "    plt.plot(range(0, len(median_train_epoch_losses)), median_train_epoch_losses, color=\"blue\")\n",
    "    plt.plot(range(0, len(median_validate_epoch_losses)), median_validate_epoch_losses, color=\"orange\")\n",
    "    plt.axhline(y = np.min(median_validate_epoch_losses), color = \"black\", linewidth=0.2, linestyle = \"dashed\")\n",
    "else:\n",
    "    plt.plot(range(0, len(mean_train_epoch_losses)), mean_train_epoch_losses, color=\"blue\")\n",
    "    plt.plot(range(0, len(mean_validate_epoch_losses)), mean_validate_epoch_losses, color=\"orange\")\n",
    "    plt.axhline(y = np.min(mean_validate_epoch_losses), color = \"black\", linewidth=0.2, linestyle = \"dashed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = 20 best with mish instead of relu (v = 21) maybe best net so far v = 23 mk = 1\n",
    "v = 24\n",
    "mk = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./ckpt.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"./whistleDistanceNetV{v}Mk{mk}_weights.h5\")\n",
    "model.save(f\"./whistleDistanceNetV{v}Mk{mk}_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_GENERATOR = data_generator(whistle_length_test, X_TEST, Y_TEST, ROOMS_TEST)\n",
    "STEPS = len(whistle_length_train[whistle_length_train>=MIN_WHISTLE_SIZE])/BATCH_SIZE\n",
    "\n",
    "error = []\n",
    "pred_distances = []\n",
    "true_distances = []\n",
    "for step, batch in enumerate(TEST_GENERATOR):\n",
    "    if step >= STEPS:\n",
    "            break\n",
    "    spectrograms, drr, distances, room = batch\n",
    "    pred_distance = model.predict([spectrograms, drr])\n",
    "    pred_distances.extend(np.argmax(pred_distance, axis=-1)/(len(BINS)-1))\n",
    "    true_distances.extend(np.argmax(distances, axis=-1)/(len(BINS)-1))\n",
    "\n",
    "pred_distance = np.array(pred_distance)\n",
    "true_distances = np.array(true_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_error = np.abs(true_distances - pred_distances)\n",
    "print(f\"Distance:\")\n",
    "print(f\"   min: {np.round(np.min(distance_error), 2)}\")\n",
    "print(f\"   5%Q: {np.round(np.quantile(distance_error, 0.05), 2)}\")\n",
    "print(f\"median: {np.round(np.median(distance_error), 2)}\")\n",
    "print(f\"  mean: {np.round(np.mean(distance_error), 2)}\")\n",
    "print(f\"  95%Q: {np.round(np.quantile(distance_error, 0.95), 2)}\")\n",
    "print(f\"   max: {np.round(np.max(distance_error), 2)}\")\n",
    "print(f\" Range: {np.round(np.quantile(distance_error, 0.95), 2) - np.round(np.quantile(distance_error, 0.05), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (7,7)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "axs.boxplot(np.squeeze(distance_error), showfliers=True)\n",
    "axs.set_xticklabels([\"Distance Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('DevilsWhistle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c5a5ffd62d732e300708fd668f1092355c1f41f7e0b1acf25039f420994e27e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
